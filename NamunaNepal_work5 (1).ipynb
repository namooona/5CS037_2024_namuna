{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOUm6KcUgell+bgOGZPJls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Building a cost function"],"metadata":{"id":"ViYytiY5Q-UH"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7MiQ9QtJXtT","executionInfo":{"status":"ok","timestamp":1735403961078,"user_tz":-345,"elapsed":470,"user":{"displayName":"Namuna Nepal","userId":"14430923077148942995"}},"outputId":"2f54d2b9-1123-4bf8-e785-c8ca6a90ce47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Proceed Further\n","Cost function output: 0.0\n"]}],"source":["import numpy as np\n","\n","# Define the cost function\n","def cost_function(X, Y, W):\n","    \"\"\"\n","    Parameters:\n","    This function finds the Mean Square Error.\n","    Input parameters:\n","    X: Feature Matrix\n","    Y: Target Matrix\n","    W: Weight Matrix\n","    Output Parameters:\n","    cost: accumulated mean square error.\n","    \"\"\"\n","\n","    #predicted values\n","    predictions = X_test.dot(W_test)\n","\n","    #calculate the mean square error\n","    cost = np.mean((predictions - Y_test) ** 2)\n","\n","    return cost\n","\n","# Test case\n","X_test = np.array([[1, 2], [3, 4], [5, 6]])\n","Y_test = np.array([3, 7, 11])\n","W_test = np.array([1, 1])\n","cost = cost_function(X_test, Y_test, W_test)\n","if cost == 0:\n","  print(\"Proceed Further\")\n","else:\n","  print(\"something went wrong: Reimplement a cost function\")\n","print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"]},{"cell_type":"markdown","source":["Gradient Descent"],"metadata":{"id":"eHRpwa9HRHkR"}},{"cell_type":"code","source":["def gradient_descent(X, Y, W, alpha, iterations):\n","    \"\"\"\n","    Perform gradient descent to optimize the parameters of a linear regression model.\n","    Parameters:\n","    X (numpy.ndarray): Feature matrix (m x n).\n","    Y (numpy.ndarray): Target vector (m x 1).\n","    W (numpy.ndarray): Initial guess for parameters (n x 1).\n","    alpha (float): Learning rate.\n","    iterations (int): Number of iterations for gradient descent.\n","    Returns:\n","    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n","    \"\"\"\n","    cost_history = []  # To store cost after each iteration\n","    m = len(Y)  # Number of samples\n","\n","    W_update = W  # Start with the initial weights\n","\n","    for iteration in range(iterations):\n","        # Step 1: Calculate predictions\n","        Y_pred = X.dot(W_update)\n","\n","        # Step 2: Calculate the difference (loss)\n","        loss = Y_pred - Y\n","\n","        # Step 3: Calculate the gradient\n","        dw = (1 / m) * X.T.dot(loss)\n","\n","        # Step 4: Update the weights\n","        W_update = W_update - alpha * dw\n","\n","        # Step 5: Calculate the cost and store it\n","        cost = cost_function(X, Y, W_update)\n","        cost_history.append(cost)\n","\n","    return W_update, cost_history\n","\n","#Test code given:\n","# Generate random test data\n","np.random.seed(0) # For reproducibility\n","X = np.random.rand(100, 3) # 100 samples, 3 features\n","Y = np.random.rand(100)\n","W = np.random.rand(3) # Initial guess for parameters\n","# Set hyperparameters\n","alpha = 0.01\n","iterations = 1000\n","# Test the gradient_descent function\n","final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n","# Print the final parameters and cost history\n","print(\"Final Parameters:\", final_params)\n","print(\"Cost History:\", cost_history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceHLnlzRRaAU","executionInfo":{"status":"ok","timestamp":1735404568896,"user_tz":-345,"elapsed":1756,"user":{"displayName":"Namuna Nepal","userId":"14430923077148942995"}},"outputId":"5ef266e5-327f-4321-f24b-1530b3b60ff7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Parameters: [0.20551667 0.54295081 0.10388027]\n","Cost History: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["Code for RMSE"],"metadata":{"id":"Ge1e0C0eRoB-"}},{"cell_type":"code","source":["def rmse(Y, Y_pred):\n","    \"\"\"\n","    This Function calculates the Root Mean Squared Error (RMSE).\n","    Input Arguments:\n","    Y: Array of actual (target) dependent variables.\n","    Y_pred: Array of predicted dependent variables.\n","    Output Arguments:\n","    rmse: Root Mean Square Error (a single number).\n","    \"\"\"\n","    # Calculate the differences between actual and predicted values\n","    errors = Y - Y_pred\n","\n","    # Calculate the mean of the squared errors\n","    mse = (errors ** 2).mean()\n","\n","    # Take the square root of the mean squared error\n","    rmse = mse ** 0.5\n","\n","    return rmse\n"],"metadata":{"id":"klL7W7H1RqYq","executionInfo":{"status":"ok","timestamp":1735405053669,"user_tz":-345,"elapsed":1712,"user":{"displayName":"Namuna Nepal","userId":"14430923077148942995"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Code for R-Squared Error:"],"metadata":{"id":"0fISkdDaUN-H"}},{"cell_type":"code","source":["def r2(Y, Y_pred):\n","    \"\"\"\n","    This Function calculates the R Squared Error (R²).\n","    Input Arguments:\n","    Y: Array of actual (target) dependent variables.\n","    Y_pred: Array of predicted dependent variables.\n","    Output Arguments:\n","    rsquared: R Squared Error (a single number).\n","    \"\"\"\n","    # Calculate the mean of actual values\n","    mean_y = np.mean(Y)\n","\n","    # Total sum of squares (variance of actual values)\n","    ss_tot = ((Y - mean_y) ** 2).sum()\n","\n","    # Residual sum of squares (variance of prediction errors)\n","    ss_res = ((Y - Y_pred) ** 2).sum()\n","\n","    # Calculate R²\n","    r2 = 1 - (ss_res / ss_tot)\n","\n","    return r2\n"],"metadata":{"id":"lt-J2T4vUSNX","executionInfo":{"status":"ok","timestamp":1735405537679,"user_tz":-345,"elapsed":1784,"user":{"displayName":"Namuna Nepal","userId":"14430923077148942995"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"O2irqYLYUlKG"}},{"cell_type":"code","source":["def main():\n","    # Step 1: Load the dataset\n","    data = pd.read_csv('student.csv')\n","\n","    # Step 2: Split the data into features (X) and target (Y)\n","    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n","    Y = data['Writing'].values  # Target: Writing marks\n","\n","    # Step 3: Split the data into training and test sets (80% train, 20% test)\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n","    W = np.zeros(X_train.shape[1])  # Initialize weights\n","    alpha = 0.00001  # Learning rate\n","    iterations = 1000  # Number of iterations for gradient descent\n","\n","    # Step 5: Perform Gradient Descent\n","    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n","\n","    # Step 6: Make predictions on the test set\n","    Y_pred = np.dot(X_test, W_optimal)\n","\n","    # Step 7: Evaluate the model using RMSE and R-Squared\n","    model_rmse = rmse(Y_test, Y_pred)\n","    model_r2 = r2(Y_test, Y_pred)\n","\n","    # Step 8: Output the results\n","    print(\"Final Weights:\", W_optimal)\n","    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n","    print(\"RMSE on Test Set:\", model_rmse)\n","    print(\"R-Squared on Test Set:\", model_r2)\n","\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"hE61y5vrUxno","executionInfo":{"status":"error","timestamp":1735405749549,"user_tz":-345,"elapsed":484,"user":{"displayName":"Namuna Nepal","userId":"14430923077148942995"}},"outputId":"cbe2a7bb-ea58-4b87-d39e-944ac28ad529"},"execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b0c72e205c95>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Execute the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-b0c72e205c95>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Step 1: Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'student.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Corrected fancy quotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Step 2: Split the data into features (X) and target (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]}]}